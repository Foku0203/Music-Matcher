import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.applications.efficientnet import preprocess_input
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from sklearn.utils import class_weight
import numpy as np
import matplotlib.pyplot as plt
import os

# ==========================================
# 1. ‚öôÔ∏è CONFIGURATION (‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤)
# ==========================================
IMG_SIZE = 224   # ‡∏Ç‡∏ô‡∏≤‡∏î‡∏†‡∏≤‡∏û‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö EfficientNet
BATCH_SIZE = 32  # ‡∏ñ‡πâ‡∏≤‡πÅ‡∏£‡∏°‡∏Å‡∏≤‡∏£‡πå‡∏î‡∏à‡∏≠‡πÄ‡∏ï‡πá‡∏° ‡πÉ‡∏´‡πâ‡∏•‡∏î‡πÄ‡∏´‡∏•‡∏∑‡∏≠ 16
EPOCHS = 40      # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏≠‡∏ö‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î (‡∏°‡∏µ Early Stopping ‡∏ä‡πà‡∏ß‡∏¢‡∏´‡∏¢‡∏∏‡∏î‡∏ñ‡πâ‡∏≤‡∏ú‡∏•‡πÑ‡∏°‡πà‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô)
DATA_DIR = 'FER2013DATA' # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö GPU
print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))

# ==========================================
# 2. üìÇ DATA GENERATORS (‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•)
# ==========================================
train_datagen = ImageDataGenerator(
    preprocessing_function=preprocess_input,
    rotation_range=20,
    width_shift_range=0.1,
    height_shift_range=0.1,
    shear_range=0.1,
    zoom_range=0.1,
    horizontal_flip=True,
    fill_mode='nearest'
)

val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)

print("\nüöÄ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û...")
train_generator = train_datagen.flow_from_directory(
    os.path.join(DATA_DIR, 'train'),
    target_size=(IMG_SIZE, IMG_SIZE),
    color_mode='rgb',
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    shuffle=True
)

validation_generator = val_datagen.flow_from_directory(
    os.path.join(DATA_DIR, 'test'),
    target_size=(IMG_SIZE, IMG_SIZE),
    color_mode='rgb',
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    shuffle=False
)

# ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Class Weights ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏™‡∏°‡∏î‡∏∏‡∏•
print("\n‚öñÔ∏è ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Class Weights...")
class_weights_val = class_weight.compute_class_weight(
    class_weight='balanced',
    classes=np.unique(train_generator.classes),
    y=train_generator.classes
)
train_class_weights = dict(enumerate(class_weights_val))
print(f"Weights: {train_class_weights}")

# ==========================================
# 3. üß† MODEL ARCHITECTURE (‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•)
# ==========================================
def build_model():
    base_model = EfficientNetB0(
        include_top=False,
        weights='imagenet',
        input_shape=(IMG_SIZE, IMG_SIZE, 3) 
    )
    
    # Freeze Base Model ‡πÑ‡∏ß‡πâ‡∏Å‡πà‡∏≠‡∏ô
    base_model.trainable = False 

    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    x = BatchNormalization()(x)  # ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£
    x = Dense(256, activation='relu')(x)
    x = Dropout(0.5)(x)          # ‡∏Å‡∏±‡∏ô Overfitting
    outputs = Dense(7, activation='softmax')(x) 

    model = Model(inputs=base_model.input, outputs=outputs)
    return model, base_model

model, base_model = build_model()

# ==========================================
# 4. üèãÔ∏è TRAINING PHASE 1: WARM-UP
# ==========================================
print("\nüî• Phase 1: Training Top Layers (Warm-up)...")
# ‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏±‡∏ß‡∏î‡πâ‡∏ß‡∏¢ Learning Rate ‡∏õ‡∏Å‡∏ï‡∏¥
model.compile(optimizer=Adam(learning_rate=0.001), 
              loss='categorical_crossentropy', 
              metrics=['accuracy'])

# ‚ö†Ô∏è ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï: ‡πÑ‡∏°‡πà‡πÉ‡∏™‡πà steps_per_epoch ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Input ran out of data
history_warmup = model.fit(
    train_generator,
    epochs=5,  
    validation_data=validation_generator,
    class_weight=train_class_weights
)

# ==========================================
# 5. üèãÔ∏è TRAINING PHASE 2: FINE-TUNING
# ==========================================
print("\nüîì Phase 2: Fine-tuning Whole Model (Unfreezing)...")
base_model.trainable = True # ‡∏õ‡∏•‡∏î‡∏•‡πá‡∏≠‡∏Ñ‡∏ó‡∏∏‡∏Å‡∏ä‡∏±‡πâ‡∏ô

# ‡πÉ‡∏ä‡πâ Learning Rate ‡∏ï‡πà‡∏≥‡∏°‡∏≤‡∏Å ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏π‡∏ô‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
model.compile(optimizer=Adam(learning_rate=1e-5), 
              loss='categorical_crossentropy', 
              metrics=['accuracy'])

# Callbacks
checkpoint = ModelCheckpoint(
    'efficientnet_fer_best.keras', 
    monitor='val_accuracy',
    save_best_only=True,
    mode='max',
    verbose=1
)

early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=8,
    restore_best_weights=True,
    verbose=1
)

reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.2,
    patience=3,
    min_lr=1e-7,
    verbose=1
)

print("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏° Fine-tuning ‡∏¢‡∏≤‡∏ß‡πÜ...")
history = model.fit(
    train_generator,
    epochs=EPOCHS, 
    validation_data=validation_generator,
    callbacks=[checkpoint, early_stopping, reduce_lr],
    class_weight=train_class_weights 
)

print("\n‚úÖ ‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå! ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")

# ==========================================
# 6. üìä PLOT RESULTS (‡∏Å‡∏£‡∏≤‡∏ü‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå)
# ==========================================
# ‡∏£‡∏ß‡∏°‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡∏ó‡∏±‡πâ‡∏á 2 ‡∏ä‡πà‡∏ß‡∏á
acc = history_warmup.history['accuracy'] + history.history['accuracy']
val_acc = history_warmup.history['val_accuracy'] + history.history['val_accuracy']
loss = history_warmup.history['loss'] + history.history['loss']
val_loss = history_warmup.history['val_loss'] + history.history['val_loss']

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(acc, label='Train Accuracy')
plt.plot(val_acc, label='Val Accuracy')
plt.axvline(x=5, color='green', linestyle='--', label='Start Fine-tuning')
plt.title('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(loss, label='Train Loss')
plt.plot(val_loss, label='Val Loss')
plt.axvline(x=5, color='green', linestyle='--', label='Start Fine-tuning')
plt.title('Loss')
plt.legend()

plt.show()